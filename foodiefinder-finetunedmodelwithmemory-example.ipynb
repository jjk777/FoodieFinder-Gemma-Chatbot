{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"modelInstanceVersion","sourceId":11372,"databundleVersionId":7771678,"modelInstanceId":5388},{"sourceType":"modelInstanceVersion","sourceId":85986,"databundleVersionId":9247152,"modelInstanceId":72246},{"sourceType":"modelInstanceVersion","sourceId":167712,"databundleVersionId":10173496,"modelInstanceId":142673}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-18T11:48:40.365803Z","iopub.execute_input":"2024-11-18T11:48:40.367123Z","iopub.status.idle":"2024-11-18T11:48:40.399248Z","shell.execute_reply.started":"2024-11-18T11:48:40.367070Z","shell.execute_reply":"2024-11-18T11:48:40.397952Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/gemma2/keras/gemma2_instruct_2b_en/1/config.json\n/kaggle/input/gemma2/keras/gemma2_instruct_2b_en/1/tokenizer.json\n/kaggle/input/gemma2/keras/gemma2_instruct_2b_en/1/metadata.json\n/kaggle/input/gemma2/keras/gemma2_instruct_2b_en/1/model.weights.h5\n/kaggle/input/gemma2/keras/gemma2_instruct_2b_en/1/assets/tokenizer/vocabulary.spm\n/kaggle/input/gemma/keras/gemma_instruct_2b_en/2/config.json\n/kaggle/input/gemma/keras/gemma_instruct_2b_en/2/tokenizer.json\n/kaggle/input/gemma/keras/gemma_instruct_2b_en/2/metadata.json\n/kaggle/input/gemma/keras/gemma_instruct_2b_en/2/model.weights.h5\n/kaggle/input/gemma/keras/gemma_instruct_2b_en/2/assets/tokenizer/vocabulary.spm\n/kaggle/input/version107_foodiefinder_kagglex_v2/keras/version107/1/variations_df.csv\n/kaggle/input/version107_foodiefinder_kagglex_v2/keras/version107/1/final_qa_pairs_dataset.csv\n/kaggle/input/version107_foodiefinder_kagglex_v2/keras/version107/1/foodie_finder_gemma/config.json\n/kaggle/input/version107_foodiefinder_kagglex_v2/keras/version107/1/foodie_finder_gemma/preprocessor.json\n/kaggle/input/version107_foodiefinder_kagglex_v2/keras/version107/1/foodie_finder_gemma/tokenizer.json\n/kaggle/input/version107_foodiefinder_kagglex_v2/keras/version107/1/foodie_finder_gemma/metadata.json\n/kaggle/input/version107_foodiefinder_kagglex_v2/keras/version107/1/foodie_finder_gemma/model.weights.h5\n/kaggle/input/version107_foodiefinder_kagglex_v2/keras/version107/1/foodie_finder_gemma/task.json\n/kaggle/input/version107_foodiefinder_kagglex_v2/keras/version107/1/foodie_finder_gemma/assets/tokenizer/vocabulary.spm\n","output_type":"stream"}],"execution_count":65},{"cell_type":"markdown","source":"# Adding Memory: ChatState","metadata":{}},{"cell_type":"code","source":"%%time\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\" \n\nimport keras\nimport keras_nlp\nfrom IPython.display import Markdown, display\nimport textwrap","metadata":{"execution":{"iopub.status.busy":"2024-11-18T11:48:42.384071Z","iopub.execute_input":"2024-11-18T11:48:42.384553Z","iopub.status.idle":"2024-11-18T11:48:42.392815Z","shell.execute_reply.started":"2024-11-18T11:48:42.384505Z","shell.execute_reply":"2024-11-18T11:48:42.391610Z"},"trusted":true},"outputs":[{"name":"stdout","text":"CPU times: user 278 ¬µs, sys: 6 ¬µs, total: 284 ¬µs\nWall time: 291 ¬µs\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"%%time\n# my custom model\ngemma_finetuned = keras_nlp.models.GemmaCausalLM.from_preset(\"/kaggle/input/version107_foodiefinder_kagglex_v2/keras/version107/1/foodie_finder_gemma\")  # gemma_instruct_2b_en","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T11:48:43.594100Z","iopub.execute_input":"2024-11-18T11:48:43.594616Z","iopub.status.idle":"2024-11-18T11:50:38.791381Z","shell.execute_reply.started":"2024-11-18T11:48:43.594564Z","shell.execute_reply":"2024-11-18T11:50:38.789975Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 1min 7s, sys: 1min 18s, total: 2min 26s\nWall time: 1min 55s\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"def display_chat(prompt, response):\n  '''Displays an LLM prompt and response in a pretty way.'''\n  prompt = prompt.replace('\\n\\n','<br><br>')\n  prompt = prompt.replace('\\n','<br>')\n  formatted_prompt = \"<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>\" + prompt + \"</blockquote></font>\"\n  response = response.replace('‚Ä¢', '  *')\n  response = textwrap.indent(response, '', predicate=lambda _: True)\n  response = response.replace('\\n\\n','<br><br>')\n  response = response.replace('\\n','<br>')\n  response = response.replace(\"```\",\"\")\n  formatted_text = \"<font size='+1' color='teal'>ü§ñ<blockquote>\" + response + \"</blockquote></font>\"\n  return Markdown(formatted_prompt+formatted_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T11:50:38.793819Z","iopub.execute_input":"2024-11-18T11:50:38.794221Z","iopub.status.idle":"2024-11-18T11:50:38.804959Z","shell.execute_reply.started":"2024-11-18T11:50:38.794179Z","shell.execute_reply":"2024-11-18T11:50:38.803719Z"}},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":"## Interaction without memory","metadata":{}},{"cell_type":"code","source":"template = \"Question:\\n{question}\\n\\nAnswer:\\n{answer}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T10:18:55.425972Z","iopub.execute_input":"2024-11-18T10:18:55.427810Z","iopub.status.idle":"2024-11-18T10:18:55.434928Z","shell.execute_reply.started":"2024-11-18T10:18:55.427750Z","shell.execute_reply":"2024-11-18T10:18:55.433634Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"%%time\nprompt = template.format(\n    question=\"What are some restaurants that sell good tempura in Japan?\",\n    answer=\"\",\n)\ncompletion = gemma_finetuned.generate(prompt, max_length=1024)\nresponse = completion.replace(prompt, \"\")\ndisplay_chat(prompt, response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T10:19:02.804434Z","iopub.execute_input":"2024-11-18T10:19:02.805582Z","iopub.status.idle":"2024-11-18T10:22:43.400314Z","shell.execute_reply.started":"2024-11-18T10:19:02.805518Z","shell.execute_reply":"2024-11-18T10:22:43.398945Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>Question:\nWhat are some restaurants that sell good tempura in Japan?\n\nAnswer:\n</blockquote></font><font size='+1' color='teal'>ü§ñ\n\n> Tempura Iseya in Ueno, Tokyo, is a well-known tempura restaurant where diners can watch the chef prepare freshly fried tempura right before their eyes. The light batter enhances the natural flavors of fresh, seasonal ingredients, including seafood and vegetables. No reservations are required, but it can get busy. Be sure to enjoy the tempura while it's hot, adjusting the flavor with dipping sauce or salt according to your preference.\n</font>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"%%time\nprompt = template.format(\n    question=\"What type of food did I just ask about?\",\n    answer=\"\",\n)\ncompletion = gemma_finetuned.generate(prompt, max_length=1024)\nresponse = completion.replace(prompt, \"\")\ndisplay_chat(prompt, response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T10:23:14.277524Z","iopub.execute_input":"2024-11-18T10:23:14.278630Z","iopub.status.idle":"2024-11-18T10:24:28.279628Z","shell.execute_reply.started":"2024-11-18T10:23:14.278576Z","shell.execute_reply":"2024-11-18T10:24:28.278333Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 3min 35s, sys: 1.6 s, total: 3min 36s\nWall time: 1min 13s\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>Question:\nWhat type of food did I just ask about?\n\nAnswer:\n</blockquote></font><font size='+1' color='teal'>ü§ñ\n\n> You are not provided with details about the restaurant you are asking about.\n</font>"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"## Interaction with memory","metadata":{}},{"cell_type":"code","source":"class ChatState():\n  \"\"\"\n  Manages the conversation history for a turn-based chatbot\n  Follows the turn-based conversation guidelines for the Gemma family of models\n  documented at https://ai.google.dev/gemma/docs/formatting\n  \"\"\"\n  def __init__(self, model):\n    \"\"\"\n    Initializes the chat state.\n    Args:\n        model: The language model to use for generating responses.\n        system: (Optional) System instructions or bot description.\n    \"\"\"\n    self.model = model\n    self.tokenizer = keras_nlp.models.GemmaTokenizer.from_preset(\"gemma2_instruct_2b_en\") # /kaggle/input/version107_foodiefinder_kagglex_v2/keras/version107/1/foodie_finder_gemma\n    self.history = []\n  def add_to_history_as_user(self, message):\n    \"\"\"\n    Adds a user message to the history with start/end turn markers.\n    \"\"\"\n    self.history.append(message)\n  def add_to_history_as_model(self, message):\n    \"\"\"\n    Adds a model response to the history with start/end turn markers.\n    \"\"\"\n    # remove new lines\n    message = message.replace(\"\\n\",\" \")\n    self.history.append(message )\n  def get_history(self):\n    \"\"\"\n    Returns the entire chat history as a single string.\n    \"\"\"\n    return \"\".join([*self.history])\n  def get_history_blurb(self):\n    \"\"\"\n    Returns what to insert into the current prompt\n    \"\"\"\n    if len(self.history)==0:\n      return \"\"\n    else:\n      return \\\nf\"\"\"\\n\\nUse the following chat history context to respond to the instruction below:\\n\"\"\"\\\nf\"\"\"{self.get_history()}\"\"\"\n\n  def get_full_prompt(self):\n    \"\"\"\n    Builds the prompt for the language model, including history and system description.\n    \"\"\"\n    prompt = self.get_history()\n    return prompt\n  def send_message(self, message):\n    \"\"\"\n    Handles sending a user message and getting a model response.\n    Args:\n        message: The user's message.\n    Returns:\n        The model's response.\n    \"\"\"\n    # Step 2: Fake retrieving context from Chroma\n    #chroma_context = \"Some people are allergic to aspirin. \"\n    # Step 3: Fake retrieving web search context\n    #web_context = \"Many drugs have harmful interactions if taken together. \"\n    # Step 4: Construct prompt with both Chroma and web search contexts\n    prompt = self.get_full_prompt()\n    full_prompt = \\\nf\"\"\"You are an AI assistant that responds to instructions about food\"\"\"\\\nf\"\"\"{self.get_history_blurb()}\\n\"\"\"\\\nf\"\"\"Instruction: {message}\"\"\"\\\nf\"\"\"Response:\"\"\"\n    # GW for debugging - print(\"--->\\n\" + full_prompt + \"<--\")\n    self.add_to_history_as_user(message)\n    # Generate response with full prompt\n    response = self.model.generate(full_prompt, max_length=1024)\n    # GW for debugging - print(\"--->\\n\" + response + \"<--\")\n    result = response.replace(full_prompt, \"\")  # Extract only the new response\n    # Add the result to chat history\n    self.add_to_history_as_model(result)\n\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T11:50:38.807335Z","iopub.execute_input":"2024-11-18T11:50:38.807890Z","iopub.status.idle":"2024-11-18T11:50:38.832517Z","shell.execute_reply.started":"2024-11-18T11:50:38.807833Z","shell.execute_reply":"2024-11-18T11:50:38.831275Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"chat = ChatState(gemma_finetuned)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T11:50:38.835533Z","iopub.execute_input":"2024-11-18T11:50:38.836539Z","iopub.status.idle":"2024-11-18T11:50:40.952264Z","shell.execute_reply.started":"2024-11-18T11:50:38.836473Z","shell.execute_reply":"2024-11-18T11:50:40.951204Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"%%time\n# Can I find tempura in Japan? \nprompt = \"Can I find tempura in Japan?\"\nresponse = chat.send_message(prompt)\ndisplay_chat(prompt, response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T11:50:40.953651Z","iopub.execute_input":"2024-11-18T11:50:40.954026Z","iopub.status.idle":"2024-11-18T11:52:49.609567Z","shell.execute_reply.started":"2024-11-18T11:50:40.953988Z","shell.execute_reply":"2024-11-18T11:52:49.608091Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 5min 7s, sys: 1.96 s, total: 5min 9s\nWall time: 2min 8s\n","output_type":"stream"},{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>Can I find tempura in Japan?</blockquote></font><font size='+1' color='teal'>ü§ñ<blockquote>While not as widely served as in past years, tempura is still found in many restaurants in Japan, with Kyoto and Osaka being the most prolific regions with multiple restaurants in each region.</blockquote></font>"},"metadata":{}}],"execution_count":71},{"cell_type":"code","source":"%%time\nprompt = \"What food are we discussing about?\"\nresponse = chat.send_message(prompt)\ndisplay_chat(prompt, response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T11:52:49.611496Z","iopub.execute_input":"2024-11-18T11:52:49.612044Z","iopub.status.idle":"2024-11-18T11:54:03.731915Z","shell.execute_reply.started":"2024-11-18T11:52:49.611989Z","shell.execute_reply":"2024-11-18T11:54:03.730519Z"}},"outputs":[{"name":"stdout","text":"CPU times: user 3min 36s, sys: 1.63 s, total: 3min 37s\nWall time: 1min 14s\n","output_type":"stream"},{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"<font size='+1' color='brown'>üôã‚Äç‚ôÇÔ∏è<blockquote>What food are we discussing about?</blockquote></font><font size='+1' color='teal'>ü§ñ<blockquote> The text \"Tempura\" was not found in the provided context.</blockquote></font>"},"metadata":{}}],"execution_count":72},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}