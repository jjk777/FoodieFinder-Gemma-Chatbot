{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9930589,"sourceType":"datasetVersion","datasetId":5950370},{"sourceId":11371,"sourceType":"modelInstanceVersion","modelInstanceId":5171,"modelId":3533},{"sourceId":11372,"sourceType":"modelInstanceVersion","modelInstanceId":5388,"modelId":3533},{"sourceId":11373,"sourceType":"modelInstanceVersion","modelInstanceId":5391,"modelId":3533}],"dockerImageVersionId":30776,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-11-17T16:18:01.655910Z","iopub.execute_input":"2024-11-17T16:18:01.656309Z","iopub.status.idle":"2024-11-17T16:18:02.981366Z","shell.execute_reply.started":"2024-11-17T16:18:01.656264Z","shell.execute_reply":"2024-11-17T16:18:02.980258Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LOAD THE LIBRARIES","metadata":{}},{"cell_type":"code","source":"%%time\nimport os\n\nos.environ[\"KERAS_BACKEND\"] = \"jax\"  # Or \"torch\" or \"tensorflow\".\n# Avoid memory fragmentation on JAX backend.\nos.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"\n\nimport keras\nimport keras_nlp\nimport json\nimport glob\nimport kagglehub\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:25:56.508637Z","iopub.execute_input":"2024-11-12T15:25:56.509001Z","iopub.status.idle":"2024-11-12T15:26:07.85067Z","shell.execute_reply.started":"2024-11-12T15:25:56.508966Z","shell.execute_reply":"2024-11-12T15:26:07.849663Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LOAD THE GEMMA MODEL","metadata":{}},{"cell_type":"code","source":"%%time\n#gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\") # didn't work well\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_2b_en\")\n#gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_7b_en\") # can't allocate memory for this model","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:51:44.830566Z","iopub.execute_input":"2024-11-12T14:51:44.831494Z","iopub.status.idle":"2024-11-12T14:51:44.835455Z","shell.execute_reply.started":"2024-11-12T14:51:44.831441Z","shell.execute_reply":"2024-11-12T14:51:44.834536Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-11-12T14:51:49.652555Z","iopub.execute_input":"2024-11-12T14:51:49.653468Z","iopub.status.idle":"2024-11-12T14:51:49.657391Z","shell.execute_reply.started":"2024-11-12T14:51:49.653427Z","shell.execute_reply":"2024-11-12T14:51:49.656396Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LOAD THE DATA","metadata":{}},{"cell_type":"code","source":"# wildcard to match all JSON files in the folder. Note- this is the synthetic data from chatgpt\njson_files = glob.glob('/kaggle/input/qa-json-pairs-final/*.json')\nprint(json_files)\n\ndataframes = []\n\nfor file in json_files[1:4]:\n    df = pd.read_json(file)\n    dataframes.append(df)\n\nqa_pairs = pd.concat(dataframes, ignore_index=True)\n\n# load the qa dataset generated using \"FoodieFinder_GenerateQADataset\" Kaggle notebook\nqa_pairs_cat_data = pd.read_csv(\"/kaggle/input/qa-json-pairs-final/qa_df.csv\")\nqa_pairs_cat_data.rename(columns={\"Question\": \"question\", \"Answer\": \"answer\"}, inplace=True)\n\nqa_pairs = pd.concat([qa_pairs,qa_pairs_cat_data])\nqa_pairs.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:01:44.605422Z","iopub.execute_input":"2024-11-14T16:01:44.606516Z","iopub.status.idle":"2024-11-14T16:01:44.673318Z","shell.execute_reply.started":"2024-11-14T16:01:44.606473Z","shell.execute_reply":"2024-11-14T16:01:44.672444Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"qa_pairs.head()","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:01:51.603829Z","iopub.execute_input":"2024-11-14T16:01:51.604201Z","iopub.status.idle":"2024-11-14T16:01:51.617087Z","shell.execute_reply.started":"2024-11-14T16:01:51.604163Z","shell.execute_reply":"2024-11-14T16:01:51.616164Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# recommended_sushi_places.json has list values in it, therefore loading the data in for this file differently\nwith open(\"/kaggle/input/qa-json-pairs-final/recommended_sushi_places.json\", \"r\") as file:\n    data = json.load(file)\n\n# Flatten the 'answer' field if it's a list and create a list of dictionaries\nflattened_data = []\nfor entry in data['qa_pairs']:\n    question = entry['question']\n    # If the answer is a list, join it into a single string\n    if isinstance(entry['answer'], list):\n        answer = \" \".join(entry['answer'])\n    else:\n        answer = entry['answer']\n    flattened_data.append({'question': question, 'answer': answer})\n\n# Convert to DataFrame\nsushi_df = pd.DataFrame(flattened_data)\n\nqa_pairs = pd.concat([qa_pairs,sushi_df])","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:01:51.803234Z","iopub.execute_input":"2024-11-14T16:01:51.803631Z","iopub.status.idle":"2024-11-14T16:01:51.813482Z","shell.execute_reply.started":"2024-11-14T16:01:51.803572Z","shell.execute_reply":"2024-11-14T16:01:51.812687Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"qa_pairs.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:01:52.042839Z","iopub.execute_input":"2024-11-14T16:01:52.043187Z","iopub.status.idle":"2024-11-14T16:01:52.049307Z","shell.execute_reply.started":"2024-11-14T16:01:52.043153Z","shell.execute_reply":"2024-11-14T16:01:52.048421Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ADD VARIATIONS TO THE QA DATASET","metadata":{}},{"cell_type":"code","source":"# Create question templates with variations to train the model on rephrased versions of the same question.\nquestion_patterns = {\n    \"What are some recommended dishes to try\": [\n        \"Can you suggest some dishes to try\",\n        \"What dishes would you recommend\",\n        \"Which dishes are a must-try\",\n    ],\n    \"Recommend a place to try\": [\n        \"Could you suggest a place to try\",\n        \"What are some good places to try\",\n        \"Where should I go to try this\",\n    ],\n    \"What are some restaurants\": [\n        \"Could you recommend some restaurants\",\n        \"What are a few good restaurants\",\n        \"Any recommendations for restaurants\",\n    ],\n    \"What are some good places to try\": [\n        \"Where can I find some good places to try\",\n        \"Can you recommend some good places\",\n        \"What are a few top places to try\",\n    ],\n    \"What restaurants are known for\": [\n        \"Which restaurants are famous for\",\n        \"Do you know of any restaurants known for\",\n        \"Are there restaurants that specialize in\",\n    ],\n    \"What is the average rating of\": [\n        \"How is the average rating for\",\n        \"What's the rating like for\",\n        \"Can you tell me the average rating of\",\n    ],\n    \"What is the location of\": [\n        \"Where is it located?\",\n        \"Can you share the location of\",\n        \"Where can I find\",\n    ],\n    \"What is the full address of\": [\n        \"Could you provide the full address for\",\n        \"What's the complete address of\",\n        \"Can I have the full address of\",\n    ],\n    \"What is the price normally spent for dining at the restaurant\": [\n        \"What's the average cost for dining at\",\n        \"How much is usually spent for a meal at\",\n        \"Can you tell me the typical price range for dining at\",\n    ],\n    \"Recommend a restaurant that specializes in\": [\n        \"Could you suggest a place that specializes in\",\n        \"Do you know any restaurants that offer\",\n        \"Where can I go for a restaurant that serves\",\n    ],\n    \"Tell me something about the restaurant\": [\n        \"Can you share some information about\",\n        \"What should I know about\",\n        \"Could you give me some details about\",\n    ]\n}\n\n# Generate variations\nqa_pairs_with_variations = []\n\nfor idx, row in qa_pairs.iterrows():\n    question = row['question']\n    answer = row['answer']\n\n    for pattern, variations in question_patterns.items():\n        if question.startswith(pattern):\n            # Generate variations based on the pattern\n            for variation in variations:\n                # Replace the pattern with the variation in the question text\n                rephrased_question = question.replace(pattern, variation, 1)\n                qa_pairs_with_variations.append({\"question\": rephrased_question, \"answer\": answer})\n            break\n    else:\n        # If no pattern matches, keep the original\n        qa_pairs_with_variations.append({\"question\": question, \"answer\": answer})\n\n# Convert to DataFrame and save as new CSV\nvariations_df = pd.DataFrame(qa_pairs_with_variations)\nvariations_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:01:54.732652Z","iopub.execute_input":"2024-11-14T16:01:54.733551Z","iopub.status.idle":"2024-11-14T16:01:54.791789Z","shell.execute_reply.started":"2024-11-14T16:01:54.73351Z","shell.execute_reply":"2024-11-14T16:01:54.790837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"variations_df.to_csv(\"variations_df.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:02:14.983124Z","iopub.execute_input":"2024-11-14T16:02:14.983761Z","iopub.status.idle":"2024-11-14T16:02:15.006049Z","shell.execute_reply.started":"2024-11-14T16:02:14.983721Z","shell.execute_reply":"2024-11-14T16:02:15.005161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"qa_pairs = pd.concat([qa_pairs,variations_df])\nqa_pairs.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-14T16:02:15.703113Z","iopub.execute_input":"2024-11-14T16:02:15.703494Z","iopub.status.idle":"2024-11-14T16:02:15.710317Z","shell.execute_reply.started":"2024-11-14T16:02:15.703454Z","shell.execute_reply":"2024-11-14T16:02:15.709438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"qa_pairs.dropna(inplace=True)\nqa_pairs.shape","metadata":{"execution":{"iopub.status.busy":"2024-11-14T16:02:16.833408Z","iopub.execute_input":"2024-11-14T16:02:16.833808Z","iopub.status.idle":"2024-11-14T16:02:16.843125Z","shell.execute_reply.started":"2024-11-14T16:02:16.833768Z","shell.execute_reply":"2024-11-14T16:02:16.842274Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CHECK AVERAGE TOKEN LENGTH","metadata":{}},{"cell_type":"markdown","source":"Below is a function to tokenize and compute average length of token. Will be useful to set the sequence length parameter while fine tuning gemma.","metadata":{}},{"cell_type":"code","source":"# %%time\n# from keras_nlp.models import GemmaTokenizer\n\n# # Load the tokenizer specific to your Gemma model (assuming 'gemma_2b_en')\n# tokenizer = GemmaTokenizer.from_preset('gemma_2b_en')\n\n# # function to tokenize and compute average length\n# def get_average_token_length(qa_dataset):\n#     token_lengths = []\n#     for index, row in qa_dataset.iterrows():\n#         # Combine question and answer to tokenize them together\n#         text = row['question'] + \" \" + row['answer']\n        \n#         # Tokenize using Gemma's tokenizer\n#         tokens = tokenizer.tokenize(text)\n        \n#         # Append the token length of the current row\n#         token_lengths.append(len(tokens))\n    \n#     # Return the average token length\n#     return sum(token_lengths) / len(token_lengths) if len(token_lengths) > 0 else 0\n\n\n# # Calculate the average token length\n# average_length = get_average_token_length(qa_pairs)\n# print(f\"Average token length: {average_length}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:28:52.333442Z","iopub.execute_input":"2024-11-12T15:28:52.334261Z","iopub.status.idle":"2024-11-12T15:28:52.340173Z","shell.execute_reply.started":"2024-11-12T15:28:52.334202Z","shell.execute_reply":"2024-11-12T15:28:52.339162Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"qa_pairs.to_csv(\"final_qa_pairs_dataset.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:28:58.522727Z","iopub.execute_input":"2024-11-12T15:28:58.52406Z","iopub.status.idle":"2024-11-12T15:28:58.551726Z","shell.execute_reply.started":"2024-11-12T15:28:58.524018Z","shell.execute_reply":"2024-11-12T15:28:58.550944Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DEEFINE THE TEMPLATE","metadata":{}},{"cell_type":"code","source":"template = \"Question:\\n{question}\\n\\nAnswer:\\n{answer}\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = []\nfor index, row in qa_pairs.iterrows():\n    formatted_string = template.format(question=row['question'], answer=row['answer'])\n    data.append(formatted_string)","metadata":{"execution":{"iopub.status.busy":"2024-11-12T15:29:00.452577Z","iopub.execute_input":"2024-11-12T15:29:00.452957Z","iopub.status.idle":"2024-11-12T15:29:00.596104Z","shell.execute_reply.started":"2024-11-12T15:29:00.452916Z","shell.execute_reply":"2024-11-12T15:29:00.595328Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LoRA FINE TUNING","metadata":{}},{"cell_type":"code","source":"# Enable LoRA for the model and set the LoRA rank to 5\ngemma_lm.backbone.enable_lora(rank=5)\ngemma_lm.summary()","metadata":{"execution":{"iopub.status.busy":"2024-10-28T14:07:44.912126Z","iopub.execute_input":"2024-10-28T14:07:44.912976Z","iopub.status.idle":"2024-10-28T14:07:45.663554Z","shell.execute_reply.started":"2024-10-28T14:07:44.912937Z","shell.execute_reply":"2024-10-28T14:07:45.662657Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\ngemma_lm.preprocessor.sequence_length = 120 #100,512,256\n\noptimizer = keras.optimizers.AdamW(\n    learning_rate = 4e-5, # 2e-4, 3e-4\n    weight_decay = 0.01, #0.02\n)\n\n\noptimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n\n\ngemma_lm.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    optimizer=optimizer,\n    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T08:58:02.793025Z","iopub.execute_input":"2024-10-26T08:58:02.7934Z","iopub.status.idle":"2024-10-26T08:58:02.805308Z","shell.execute_reply.started":"2024-10-26T08:58:02.793366Z","shell.execute_reply":"2024-10-26T08:58:02.804359Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It takes 10 hours 30 mins for fine tuning the model.","metadata":{}},{"cell_type":"code","source":"%%time\nhistory = gemma_lm.fit(data, epochs=90, batch_size=5)  # 100,60","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:11:41.600036Z","iopub.execute_input":"2024-10-26T09:11:41.6004Z","iopub.status.idle":"2024-10-26T09:27:29.577128Z","shell.execute_reply.started":"2024-10-26T09:11:41.600361Z","shell.execute_reply":"2024-10-26T09:27:29.576144Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot accuracy\nplt.plot(history.history['sparse_categorical_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper left')\nplt.show()\n\n# Plot loss\nplt.plot(history.history['loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train'], loc='upper left')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:27:29.579889Z","iopub.execute_input":"2024-10-26T09:27:29.580334Z","iopub.status.idle":"2024-10-26T09:27:30.124131Z","shell.execute_reply.started":"2024-10-26T09:27:29.580298Z","shell.execute_reply":"2024-10-26T09:27:30.123186Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install -q -U kagglehub --upgrade # run if required","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SAVE THE MODEL TO OUTPUT DIRECTORY","metadata":{}},{"cell_type":"code","source":"%%time\n\n## Save the finetuned model as a KerasNLP preset.\npreset_dir = \"./foodie_finder_gemma\"\ngemma_lm.save_to_preset(preset_dir)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:36:27.361095Z","iopub.execute_input":"2024-10-26T09:36:27.361485Z","iopub.status.idle":"2024-10-26T09:37:03.4266Z","shell.execute_reply.started":"2024-10-26T09:36:27.361449Z","shell.execute_reply":"2024-10-26T09:37:03.425338Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Note:** After saving the model, I uploaded my model manually instead of running the 2 blocks of code below.","metadata":{}},{"cell_type":"markdown","source":"# UPLOAD THE MODEL TO KAGGLE HUB","metadata":{}},{"cell_type":"code","source":"# import kagglehub\n# kagglehub.login()","metadata":{"execution":{"iopub.status.busy":"2024-11-03T10:16:46.55153Z","iopub.execute_input":"2024-11-03T10:16:46.551904Z","iopub.status.idle":"2024-11-03T10:16:46.556365Z","shell.execute_reply.started":"2024-11-03T10:16:46.551867Z","shell.execute_reply":"2024-11-03T10:16:46.55532Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%time\n# # upload to kagglehub\n# kaggle_uri = f\"kaggle://kjeevan/foodie_finder_gemma/keras/foodie_finder_gemma\"\n# keras_nlp.upload_preset(kaggle_uri, preset_dir)","metadata":{"execution":{"iopub.status.busy":"2024-11-03T10:16:43.6925Z","iopub.execute_input":"2024-11-03T10:16:43.692914Z","iopub.status.idle":"2024-11-03T10:16:43.697636Z","shell.execute_reply.started":"2024-11-03T10:16:43.692873Z","shell.execute_reply":"2024-11-03T10:16:43.696335Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CODE TO UPLOAD THE MODEL TO HUGGING FACE","metadata":{}},{"cell_type":"code","source":"# from huggingface_hub import HfApi\n# from huggingface_hub import login\n# from huggingface_hub import upload_folder\n\n# # Paste the access token or use secrets from \"Add-ons\" for best practice\n# login(\"the_access_token_from_hf\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# repo_name = \"Jeevan18/FoodieFinderV2\"  # Change to your desired repository name\n# HfApi().create_repo(repo_name, exist_ok=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%time\n\n# # Directory where the fine tuned model is saved\n# model_dir = \"/kaggle/input/version107_foodiefinder_kagglex_v2/keras/version107/1/foodie_finder_gemma\"\n\n# # Upload all files in the model directory to the Hugging Face Hub\n# upload_folder(\n#     repo_id=\"Jeevan18/FoodieFinderV2\",  # the repository name\n#     folder_path=model_dir,\n#     #path_in_repo=\" \",  # Upload to the root of the repo\n#     commit_message=\"Initial model upload\",\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TESTING THE FINE TUNED MODEL","metadata":{}},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"What are some tips for visiting ramen shops in Japan?\",\n    answer=\"\" \n)\n\n# Generate the answer using the fine-tuned Gemma model\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:30:22.36049Z","iopub.execute_input":"2024-10-26T09:30:22.360893Z","iopub.status.idle":"2024-10-26T09:30:24.992075Z","shell.execute_reply.started":"2024-10-26T09:30:22.360854Z","shell.execute_reply":"2024-10-26T09:30:24.991006Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"What are the different food types served in Tokyo?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:30:24.993583Z","iopub.execute_input":"2024-10-26T09:30:24.993872Z","iopub.status.idle":"2024-10-26T09:30:26.485231Z","shell.execute_reply.started":"2024-10-26T09:30:24.993841Z","shell.execute_reply":"2024-10-26T09:30:26.484216Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"What are some recommended sushi places to try in Japan?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:31:24.392171Z","iopub.execute_input":"2024-10-26T09:31:24.392576Z","iopub.status.idle":"2024-10-26T09:31:27.614737Z","shell.execute_reply.started":"2024-10-26T09:31:24.392539Z","shell.execute_reply":"2024-10-26T09:31:27.613772Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"What makes Tokyo Sushi Ginza Sushi-Ichi special?\",\n    answer=\"\"\n)\n\n# Generate the answer using the fine-tuned Gemma model\nprint(gemma_lm.generate(prompt, max_length=800))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:31:27.616484Z","iopub.execute_input":"2024-10-26T09:31:27.616789Z","iopub.status.idle":"2024-10-26T09:31:29.297809Z","shell.execute_reply.started":"2024-10-26T09:31:27.616757Z","shell.execute_reply":"2024-10-26T09:31:29.296848Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"What are some must-visit ramen spots in Japan?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:32:09.165765Z","iopub.execute_input":"2024-10-26T09:32:09.166121Z","iopub.status.idle":"2024-10-26T09:32:11.636801Z","shell.execute_reply.started":"2024-10-26T09:32:09.166084Z","shell.execute_reply":"2024-10-26T09:32:11.63592Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"Recommend a place to enjoy a wide selection of sake in Tokyo.\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:33:27.820838Z","iopub.execute_input":"2024-10-26T09:33:27.82157Z","iopub.status.idle":"2024-10-26T09:33:30.494595Z","shell.execute_reply.started":"2024-10-26T09:33:27.82153Z","shell.execute_reply":"2024-10-26T09:33:30.493693Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"What are some recommended dishes to try in Chugoku?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=1800))\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:34:18.080352Z","iopub.execute_input":"2024-10-26T09:34:18.081132Z","iopub.status.idle":"2024-10-26T09:34:20.413798Z","shell.execute_reply.started":"2024-10-26T09:34:18.081092Z","shell.execute_reply":"2024-10-26T09:34:20.41273Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"Recommend a place to try shabushabu in Japan.\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:34:31.952384Z","iopub.execute_input":"2024-10-26T09:34:31.952787Z","iopub.status.idle":"2024-10-26T09:34:35.127942Z","shell.execute_reply.started":"2024-10-26T09:34:31.952748Z","shell.execute_reply":"2024-10-26T09:34:35.126965Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"What are some restaurants that sell good tempura in Japan?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:34:46.761608Z","iopub.execute_input":"2024-10-26T09:34:46.76201Z","iopub.status.idle":"2024-10-26T09:34:49.662649Z","shell.execute_reply.started":"2024-10-26T09:34:46.761967Z","shell.execute_reply":"2024-10-26T09:34:49.661447Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"What sets Okinawa Darumasoba apart?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:34:53.360293Z","iopub.execute_input":"2024-10-26T09:34:53.361Z","iopub.status.idle":"2024-10-26T09:34:55.856673Z","shell.execute_reply.started":"2024-10-26T09:34:53.360952Z","shell.execute_reply":"2024-10-26T09:34:55.855597Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"What are the main differences in ramen styles across Japan?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:35:04.580252Z","iopub.execute_input":"2024-10-26T09:35:04.580628Z","iopub.status.idle":"2024-10-26T09:35:07.157416Z","shell.execute_reply.started":"2024-10-26T09:35:04.580591Z","shell.execute_reply":"2024-10-26T09:35:07.156442Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"I want to eat sushi, where should I dine at in Japan?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:40:15.219609Z","iopub.execute_input":"2024-10-26T09:40:15.220406Z","iopub.status.idle":"2024-10-26T09:40:17.710423Z","shell.execute_reply.started":"2024-10-26T09:40:15.220364Z","shell.execute_reply":"2024-10-26T09:40:17.709415Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"I want to eat sushi, what is a recommended sushi place in Japan?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"execution":{"iopub.status.busy":"2024-10-26T09:42:07.551902Z","iopub.execute_input":"2024-10-26T09:42:07.552802Z","iopub.status.idle":"2024-10-26T09:42:09.634429Z","shell.execute_reply.started":"2024-10-26T09:42:07.552762Z","shell.execute_reply":"2024-10-26T09:42:09.633269Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"What restaurants are known for friendly and attentive staff?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"What restaurants are known for great service?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"What restaurants are known for warm and inviting atmosphere and cleanliness?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"Tell me something about the restaurant Tempura Yokota.\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"Recommend a restaurant that specializes in steak.\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"Recommend a restaurant that specializes in tempura.\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"Recommend a restaurant that specializes in horumon (bbq offel).\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"What are some tips for visiting ramen shops in Japan?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"What are some must-visit ramen spots in Japan?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"What are the main differences in ramen styles across Japan?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"Which restaurants are famous for friendly and attentive staff and beautiful ambiance?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"What's the average cost for dining at Ajuta?\",\n    answer=\"\" \n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"How is the average rating for Ajuta?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"Where can I find Ajuta in Japan?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"Could you provide the full address for Ajuta?\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\nprompt = template.format(\n    question=\"Can you share some information about Ajuta.\",\n    answer=\"\"\n)\n\nprint(gemma_lm.generate(prompt, max_length=800))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CODE TO ADD MEMORY","metadata":{}},{"cell_type":"markdown","source":"Note: The memory functionality has been commented out as it caused inconsistent and unreliable chatbot responses. Further experimentation might be required before making it publicly available.","metadata":{}},{"cell_type":"code","source":"# # Code to load the model from inputs section\n# gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"/kaggle/input/version107_foodiefinder_kagglex_v2/keras/version107/1/foodie_finder_gemma\")  # gemma_instruct_2b_en","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def display_chat(prompt, response):\n#   '''Displays an LLM prompt and response in a pretty way.'''\n#   prompt = prompt.replace('\\n\\n','<br><br>')\n#   prompt = prompt.replace('\\n','<br>')\n#   formatted_prompt = \"<font size='+1' color='brown'>🙋‍♂️<blockquote>\" + prompt + \"</blockquote></font>\"\n#   response = response.replace('•', '  *')\n#   response = textwrap.indent(response, '', predicate=lambda _: True)\n#   response = response.replace('\\n\\n','<br><br>')\n#   response = response.replace('\\n','<br>')\n#   response = response.replace(\"```\",\"\")\n#   formatted_text = \"<font size='+1' color='teal'>🤖<blockquote>\" + response + \"</blockquote></font>\"\n#   return Markdown(formatted_prompt+formatted_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T11:24:40.431953Z","iopub.execute_input":"2024-11-18T11:24:40.433104Z","iopub.status.idle":"2024-11-18T11:24:40.437641Z","shell.execute_reply.started":"2024-11-18T11:24:40.433058Z","shell.execute_reply":"2024-11-18T11:24:40.436507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class ChatState():\n#   \"\"\"\n#   Manages the conversation history for a turn-based chatbot\n#   Follows the turn-based conversation guidelines for the Gemma family of models\n#   documented at https://ai.google.dev/gemma/docs/formatting\n#   \"\"\"\n#   def __init__(self, model):\n#     \"\"\"\n#     Initializes the chat state.\n#     Args:\n#         model: The language model to use for generating responses.\n#         system: (Optional) System instructions or bot description.\n#     \"\"\"\n#     self.model = model\n#     self.tokenizer = keras_nlp.models.GemmaTokenizer.from_preset(\"gemma2_instruct_2b_en\") # /kaggle/input/version107_foodiefinder_kagglex_v2/keras/version107/1/foodie_finder_gemma\n#     self.history = []\n#   def add_to_history_as_user(self, message):\n#     \"\"\"\n#     Adds a user message to the history with start/end turn markers.\n#     \"\"\"\n#     self.history.append(message)\n#   def add_to_history_as_model(self, message):\n#     \"\"\"\n#     Adds a model response to the history with start/end turn markers.\n#     \"\"\"\n#     # remove new lines\n#     message = message.replace(\"\\n\",\" \")\n#     self.history.append(message )\n#   def get_history(self):\n#     \"\"\"\n#     Returns the entire chat history as a single string.\n#     \"\"\"\n#     return \"\".join([*self.history])\n#   def get_history_blurb(self):\n#     \"\"\"\n#     Returns what to insert into the current prompt\n#     \"\"\"\n#     if len(self.history)==0:\n#       return \"\"\n#     else:\n#       return \\\n# f\"\"\"\\n\\nUse the following chat history context to respond to the instruction below:\\n\"\"\"\\\n# f\"\"\"{self.get_history()}\"\"\"\n\n#   def get_full_prompt(self):\n#     \"\"\"\n#     Builds the prompt for the language model, including history and system description.\n#     \"\"\"\n#     prompt = self.get_history()\n#     return prompt\n#   def send_message(self, message):\n#     \"\"\"\n#     Handles sending a user message and getting a model response.\n#     Args:\n#         message: The user's message.\n#     Returns:\n#         The model's response.\n#     \"\"\"\n#     # Step 2: Fake retrieving context from Chroma\n#     #chroma_context = \"Some people are allergic to aspirin. \"\n#     # Step 3: Fake retrieving web search context\n#     #web_context = \"Many drugs have harmful interactions if taken together. \"\n#     # Step 4: Construct prompt with both Chroma and web search contexts\n#     prompt = self.get_full_prompt()\n#     full_prompt = \\\n# f\"\"\"You are an AI assistant that responds to instructions about food.\"\"\"\\\n# f\"\"\"{self.get_history_blurb()}\\n\"\"\"\\\n# f\"\"\"Instruction: {message}\"\"\"\\\n# f\"\"\"Response:\"\"\"\n#     # GW for debugging - print(\"--->\\n\" + full_prompt + \"<--\")\n#     self.add_to_history_as_user(message)\n#     # Generate response with full prompt\n#     response = self.model.generate(full_prompt, max_length=1024)\n#     # GW for debugging - print(\"--->\\n\" + response + \"<--\")\n#     result = response.replace(full_prompt, \"\")  # Extract only the new response\n#     # Add the result to chat history\n#     self.add_to_history_as_model(result)\n\n#     return result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# chat = ChatState(gemma_lm)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%time\n# prompt = \"Can I find tempura in Japan?\"\n# response = chat.send_message(prompt)\n# display_chat(prompt, response)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%time\n# prompt = \"What food are we discussing?\"\n# response = chat.send_message(prompt)\n# display_chat(prompt, response)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}